{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "import json\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"/workspace/nodeidx2paperid.csv\")\n",
    "    print(f\"Loaded {len(df)} IDs.\")\n",
    "    \n",
    "    # OGB mapping files usually have a column 'paper id' for the MAG ID\n",
    "    # We rename it to 'mag_id' for clarity\n",
    "    if 'paper id' in df.columns:\n",
    "        df = df.rename(columns={'paper id': 'mag_id'})\n",
    "    \n",
    "    # Ensure they are strings for the URL\n",
    "    mag_ids = df['mag_id'].astype(str).tolist()\n",
    "    \n",
    "except KeyError:\n",
    "    print(f\"Error: Columns found: {df.columns}. Please check the CSV header.\")\n",
    "    mag_ids = []\n",
    "\n",
    "# 2. Async Fetcher\n",
    "async def fetch_batch(session, ids):\n",
    "    # Join IDs with pipe | for \"OR\" logic\n",
    "    # Filter 'ids.mag' tells OpenAlex these are Microsoft Academic Graph IDs\n",
    "    ids_param = \"|\".join(ids)\n",
    "    url = f\"https://api.openalex.org/works?filter=ids.mag:{ids_param}&per-page=100&select=id,ids,title,abstract_inverted_index\"\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                return data.get('results', [])\n",
    "            elif response.status == 429:\n",
    "                # Rate limited? Wait a sec and retry (basic handling)\n",
    "                await asyncio.sleep(2)\n",
    "                return []\n",
    "            else:\n",
    "                return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "async def main(ids_list):\n",
    "    batch_size = 50 # 50 is safer for OpenAlex URL length limits than 100\n",
    "    tasks = []\n",
    "    results = []\n",
    "    \n",
    "    # Limit to 10 simultaneous connections to be polite\n",
    "    connector = aiohttp.TCPConnector(limit_per_host=10)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        # Create all tasks\n",
    "        for i in range(0, len(ids_list), batch_size):\n",
    "            batch = ids_list[i : i + batch_size]\n",
    "            tasks.append(fetch_batch(session, batch))\n",
    "        \n",
    "        # Run them with a progress bar\n",
    "        # gathered_results will be a list of lists\n",
    "        gathered_results = await tqdm.gather(*tasks)\n",
    "        \n",
    "    # Flatten the list of lists\n",
    "    for batch_result in gathered_results:\n",
    "        results.extend(batch_result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(mag_ids) > 0:\n",
    "        # CORRECT WAY for .py scripts:\n",
    "        final_data = await main(mag_ids)\n",
    "        \n",
    "        print(f\"Fetched metadata for {len(final_data)} papers.\")\n",
    "        \n",
    "        # 4. Save to JSON\n",
    "        with open('arxiv_mag_metadata.json', 'w') as f:\n",
    "            json.dump(final_data, f)\n",
    "        print(\"Saved to arxiv_mag_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reconstruct_abstract(inverted_index):\n",
    "    if not inverted_index: return \"\"\n",
    "    max_index = 0\n",
    "    for indices in inverted_index.values():\n",
    "        if indices: max_index = max(max_index, max(indices))\n",
    "    \n",
    "    reconstructed_list = [\"\"] * (max_index + 1)\n",
    "    for word, indices in inverted_index.items():\n",
    "        for idx in indices:\n",
    "            reconstructed_list[idx] = word\n",
    "    return \" \".join(reconstructed_list)\n",
    "\n",
    "def process_item(item):\n",
    "    title = item.get('title', \"\")\n",
    "    inv_index = item.get('abstract_inverted_index')\n",
    "    abstract = reconstruct_abstract(inv_index)\n",
    "    text = f\"{title}. {abstract}\" if title or abstract else \"Paper content unavailable\"\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'arxiv_mag_metadata.json'\n",
    "    output_text_file = 'reconstructed_texts.json'\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Reconstructing {len(data)} abstracts using all CPU cores...\")\n",
    "    # Use Pool to bypass GIL for string processing\n",
    "    with Pool() as p:\n",
    "        texts = list(tqdm(p.imap(process_item, data, chunksize=100), total=len(data)))\n",
    "\n",
    "    with open(output_text_file, 'w') as f:\n",
    "        json.dump(texts, f)\n",
    "    print(\"Stage 1 Complete: Text saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def reconstruct_abstract(inverted_index):\n",
    "    if not inverted_index:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Find the length of the abstract by finding the max index\n",
    "    max_index = 0\n",
    "    for indices in inverted_index.values():\n",
    "        if indices:\n",
    "            max_index = max(max_index, max(indices))\n",
    "            \n",
    "    # 2. Create an empty list of the right size\n",
    "    # We add +1 because indices are 0-based\n",
    "    reconstructed_list = [\"\"] * (max_index + 1)\n",
    "    \n",
    "    # 3. Fill the list with words\n",
    "    for word, indices in inverted_index.items():\n",
    "        for idx in indices:\n",
    "            reconstructed_list[idx] = word\n",
    "            \n",
    "    # 4. Join them back into a string\n",
    "    return \" \".join(reconstructed_list)\n",
    "\n",
    "def generate_local_embeddings(input_file, output_file):\n",
    "    # 1. Load Data\n",
    "    print(f\"Loading {input_file}...\")\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: {input_file} not found.\")\n",
    "        return\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. Prepare Text\n",
    "    # Combine Title + Abstract for best context\n",
    "    texts = []\n",
    "    for item in data:\n",
    "        title = item.get('title', \"\")\n",
    "        \n",
    "        # Handle the inverted index\n",
    "        inv_index = item.get('abstract_inverted_index')\n",
    "        if inv_index:\n",
    "            abstract = reconstruct_abstract(inv_index)\n",
    "        else:\n",
    "            abstract = \"\"\n",
    "\n",
    "        if not title and not abstract:\n",
    "            text = \"Paper content unavailable\"\n",
    "        else:\n",
    "            text = f\"{title}. {abstract}\"\n",
    "        \n",
    "        texts.append(text)\n",
    "\n",
    "    print(f\"Loaded {len(texts)} papers. Loading model...\")\n",
    "\n",
    "    model = model = SentenceTransformer(\n",
    "                        \"Qwen/Qwen3-Embedding-4B\",\n",
    "                        model_kwargs={\n",
    "                            \"attn_implementation\": \"flash_attention_2\", \n",
    "                            \"torch_dtype\": \"float16\",\n",
    "                            \"device_map\": \"auto\"\n",
    "                        }\n",
    "                    )\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cpu\" and torch.backends.mps.is_available():\n",
    "        device = \"mps\" # For Mac users\n",
    "    \n",
    "    model.to(device)\n",
    "    print(f\"Model loaded on {device}.\")\n",
    "\n",
    "    # 4. Generate Embeddings (with MRL Truncation)\n",
    "    # We can perform the truncation *after* generation or let the model handle it if supported.\n",
    "    # The safest way for MRL models is to generate full dims and slice, \n",
    "    # as the first N dimensions contain the most info.\n",
    "    \n",
    "    BATCH_SIZE = 16 # Adjust based on your VRAM (32 for 8GB, 128 for 24GB)\n",
    "    \n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        texts, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        show_progress_bar=True, \n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    print(f\"Original shape: {embeddings.shape}\")\n",
    "    embeddings_256 = embeddings[:, :256]\n",
    "    embeddings_256 = F.normalize(embeddings_256, p=2, dim=1)\n",
    "    print(f\"Truncated & Re-normalized shape: {embeddings_256.shape}\")\n",
    "\n",
    "    # 6. Save\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    torch.save(embeddings_256.cpu(), output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_local_embeddings('arxiv_mag_metadata.json', 'qwen_embeddings_256.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Files\n",
    "MAPPING_FILE = Path(\"data/ogbn_arxiv/mapping/nodeidx2paperid.csv.gz\")\n",
    "METADATA_FILE = Path(\"arxiv_mag_metadata.json\")\n",
    "EMBEDDING_FILE = Path(\"qwen_embeddings_256.pt\")\n",
    "OUTPUT_FILE = Path(\"qwen_embeddings_256_aligned.pt\")\n",
    "\n",
    "def align_embeddings():\n",
    "    print(\"1. Loading OGB Mapping (The Ground Truth)...\")\n",
    "    # This CSV maps Graph Node Index -> MAG Paper ID\n",
    "    df_mapping = pd.read_csv(MAPPING_FILE)\n",
    "    \n",
    "    # Handle column naming variations\n",
    "    if 'paper id' in df_mapping.columns:\n",
    "        df_mapping = df_mapping.rename(columns={'paper id': 'mag_id'})\n",
    "    \n",
    "    # Create a list where index 0 = mag_id for node 0, etc.\n",
    "    # We explicitly sort by node idx just to be safe, though usually it's sorted\n",
    "    if 'node idx' in df_mapping.columns:\n",
    "        df_mapping = df_mapping.sort_values('node idx')\n",
    "        \n",
    "    ground_truth_ids = df_mapping['mag_id'].astype(str).tolist()\n",
    "    total_nodes = len(ground_truth_ids)\n",
    "    print(f\"   Graph expects {total_nodes} nodes.\")\n",
    "\n",
    "    print(\"2. Loading Fetched Metadata & Embeddings...\")\n",
    "    with open(METADATA_FILE, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Helper to clean IDs (OpenAlex returns \"https://openalex.org/W213...\", we just want \"213...\")\n",
    "    def clean_id(url_or_id):\n",
    "        return str(url_or_id).replace(\"https://openalex.org/W\", \"\").replace(\"W\", \"\")\n",
    "\n",
    "    # Map the fetched MAG ID to its index in the *fetched* tensor\n",
    "    # metadata[i] corresponds to embeddings[i]\n",
    "    fetched_id_to_index = {}\n",
    "    for idx, item in enumerate(metadata):\n",
    "        # OpenAlex IDs usually look like 'https://openalex.org/W12345' or just integer IDs\n",
    "        # We try to extract the MAG ID part\n",
    "        raw_id = item.get('id', '')\n",
    "        mag_id = clean_id(raw_id)\n",
    "        \n",
    "        # Also check 'ids' field if available (sometimes MAG id is nested)\n",
    "        if 'ids' in item and 'mag' in item['ids']:\n",
    "            mag_id = str(item['ids']['mag'])\n",
    "            \n",
    "        fetched_id_to_index[mag_id] = idx\n",
    "\n",
    "    print(f\"   Mapped {len(fetched_id_to_index)} fetched papers to their tensor indices.\")\n",
    "\n",
    "    print(\"3. Loading Tensor...\")\n",
    "    # Load on CPU to save memory\n",
    "    raw_embeddings = torch.load(EMBEDDING_FILE, map_location='cpu')\n",
    "    embedding_dim = raw_embeddings.shape[1]\n",
    "    \n",
    "    # Create the final aligned tensor (Filled with Zeros)\n",
    "    aligned_tensor = torch.zeros((total_nodes, embedding_dim), dtype=torch.float32)\n",
    "    \n",
    "    print(\"4. Aligning...\")\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    # Iterate through the GROUND TRUTH order\n",
    "    for node_idx, target_mag_id in enumerate(tqdm(ground_truth_ids)):\n",
    "        target_mag_id = str(target_mag_id)\n",
    "        \n",
    "        if target_mag_id in fetched_id_to_index:\n",
    "            # Found it! Grab the vector from the raw pile\n",
    "            raw_idx = fetched_id_to_index[target_mag_id]\n",
    "            aligned_tensor[node_idx] = raw_embeddings[raw_idx]\n",
    "            hits += 1\n",
    "        else:\n",
    "            # Missing? It stays as zeros (or you could use random noise)\n",
    "            misses += 1\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Alignment Complete.\")\n",
    "    print(f\"✅ Matched: {hits} ({hits/total_nodes:.1%})\")\n",
    "    print(f\"❌ Missing: {misses} ({misses/total_nodes:.1%}) -> Filled with zeros\")\n",
    "    print(f\"Final Tensor Shape: {aligned_tensor.shape}\")\n",
    "    \n",
    "    torch.save(aligned_tensor, OUTPUT_FILE)\n",
    "    print(f\"Saved aligned tensor to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    align_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1422e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download and load the ogbn-arxiv dataset into data/, fuse with Qwen embeddings,\n",
    "and create a contrastive LinkNeighborLoader.\n",
    "\"\"\"\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "# --- CHANGE 1: Import the PyG-specific dataset wrapper ---\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import torch_geometric.transforms as T\n",
    "import torch_sparse\n",
    "import pyg_lib\n",
    "\n",
    "# Define paths relative to this script\n",
    "# Wrap string in Path() to ensure .mkdir() works\n",
    "DATA_DIR = Path(\"./data\") \n",
    "EMBEDDING_PATH = Path(\"qwen_embeddings_256_aligned.pt\") \n",
    "\n",
    "def download_ogbn_arxiv(root: str | Path | None = None):\n",
    "    \"\"\"Download ogbn-arxiv to data/ and return the PyG dataset object.\"\"\"\n",
    "    if root is None:\n",
    "        root = DATA_DIR\n",
    "    else:\n",
    "        root = Path(root)\n",
    "        \n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading/Loading ogbn-arxiv at {root}...\")\n",
    "    # --- CHANGE 2: Use PygNodePropPredDataset instead of NodePropPredDataset ---\n",
    "    dataset = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=str(root))\n",
    "    return dataset\n",
    "\n",
    "def load_data_with_features(root: str | Path | None = None):\n",
    "    \"\"\"\n",
    "    Loads the graph and replaces/concatenates features with Qwen embeddings.\n",
    "    Returns: (dataset, data)\n",
    "    \"\"\"\n",
    "    dataset = download_ogbn_arxiv(root)\n",
    "    data = dataset[0] # Now this returns a PyG Data object, not a tuple\n",
    "    \n",
    "    # --- Feature Fusion ---\n",
    "    if EMBEDDING_PATH.exists():\n",
    "        print(f\"Found custom embeddings at {EMBEDDING_PATH}. Fusing...\")\n",
    "        \n",
    "        # Load Qwen embeddings (ensure CPU to prevent OOM during concat)\n",
    "        qwen_emb = torch.load(EMBEDDING_PATH, map_location='cpu', weights_only=True)\n",
    "        \n",
    "        # Validation: Check alignment\n",
    "        if qwen_emb.shape[0] != data.num_nodes:\n",
    "            raise ValueError(f\"Shape Mismatch! Graph has {data.num_nodes} nodes, \"\n",
    "                             f\"but embeddings have {qwen_emb.shape[0]} rows.\")\n",
    "\n",
    "        # Concatenate: [Original(128) | Qwen(256)] -> [384]\n",
    "        # data.x is already a tensor in PygNodePropPredDataset\n",
    "        data.x = torch.cat([data.x, qwen_emb], dim=1)\n",
    "        print(f\"Fused Features Shape: {data.x.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Warning: {EMBEDDING_PATH} not found. Using original features only.\")\n",
    "        \n",
    "    return dataset, data\n",
    "\n",
    "def create_contrastive_loader(dataset, data, batch_size=2048):\n",
    "    \"\"\"\n",
    "    Creates a LinkNeighborLoader for Contrastive Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Split Management\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']\n",
    "    \n",
    "    # Filter edge_index to only include edges where source node is in train_idx\n",
    "    print(\"Filtering training edges...\")\n",
    "    src, _ = data.edge_index\n",
    "    \n",
    "    # Create mask for edges where the source node is in the training set\n",
    "    train_mask = torch.isin(src, train_idx)\n",
    "    train_edge_index = data.edge_index[:, train_mask]\n",
    "    \n",
    "    print(f\"Training on {train_edge_index.shape[1]} edges (out of {data.num_edges} total).\")\n",
    "\n",
    "    # 2. Define the Loader\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=[10, 5],   # Sample 10 neighbors at hop 1, 5 at hop 2\n",
    "        edge_label_index=train_edge_index, # The \"Positive\" edges to learn from\n",
    "        neg_sampling_ratio=1.0,  # For every 1 real edge, generate 1 fake negative edge\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0, \n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    dataset, data = load_data_with_features()\n",
    "    \n",
    "    # 2. Create Loader\n",
    "    print(\"\\nInitializing Contrastive Loader...\")\n",
    "    train_loader = create_contrastive_loader(dataset, data)\n",
    "    \n",
    "    # 3. Test a single batch\n",
    "    print(\"\\n--- Batch Inspection ---\")\n",
    "    batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"Batch Type: {type(batch)}\")\n",
    "    print(f\"Batch Nodes: {batch.num_nodes}\") \n",
    "    print(f\"Batch Features: {batch.x.shape}\") \n",
    "    \n",
    "    print(f\"Contrastive Pairs: {batch.edge_label_index.shape}\")\n",
    "    print(f\"Labels (1=Pos, 0=Neg): {batch.edge_label[:10]}\")\n",
    "    \n",
    "    print(\"\\n✅ Setup Complete. Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Contrastive GNN Training with Custom Architecture\n",
    "# This script trains a GraphSAGE encoder using a contrastive Link Prediction task.\n",
    "# It fuses Word2Vec features (128d) with Qwen Embeddings (256d) for a total input of 384d.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, BatchNorm\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Configuration & Constants\n",
    "\n",
    "# %%\n",
    "# Hyperparameters\n",
    "INPUT_DIM = 384   # 128 (Original) + 256 (Qwen)\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 256\n",
    "BATCH_SIZE = 2048\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "EMBEDDING_PATH = Path(\"qwen_embeddings_256_aligned.pt\") # Ensure this is the ALIGNED version\n",
    "MODEL_SAVE_PATH = \"gnn_contrastive_v2.pth\"\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Loading & Feature Fusion\n",
    "# This function loads the OGB-Arxiv graph and concatenates the Qwen embeddings.\n",
    "\n",
    "# %%\n",
    "def load_data():\n",
    "    print(f\"Loading data from {DATA_DIR}...\")\n",
    "    dataset = PygNodePropPredDataset(name=\"ogbn-arxiv\", root=str(DATA_DIR))\n",
    "    data = dataset[0]\n",
    "    \n",
    "    # Feature Fusion\n",
    "    if EMBEDDING_PATH.exists():\n",
    "        print(f\"Fusing embeddings from {EMBEDDING_PATH}...\")\n",
    "        qwen_emb = torch.load(EMBEDDING_PATH, map_location='cpu', weights_only=True)\n",
    "        \n",
    "        # Validation\n",
    "        if qwen_emb.shape[0] != data.num_nodes:\n",
    "            raise ValueError(f\"Mismatch: Graph has {data.num_nodes} nodes, embeddings have {qwen_emb.shape[0]}\")\n",
    "            \n",
    "        # Concatenate: [N, 128] + [N, 256] -> [N, 384]\n",
    "        data.x = torch.cat([data.x, qwen_emb], dim=1)\n",
    "        print(f\"New Feature Shape: {data.x.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: Custom embeddings not found. Using original features only.\")\n",
    "        \n",
    "    return dataset, data\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Model Architecture\n",
    "# Using the specific V2 architecture provided.\n",
    "\n",
    "# %%\n",
    "class EmbedderGNNv2(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        # Layer 1: Input -> Hidden\n",
    "        self.convs.append(SAGEConv(in_dim, hidden_dim, aggr='mean'))\n",
    "        self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Learnable Mask Token (Replacing Node 0's features)\n",
    "        self.mask_embed = torch.nn.Parameter(torch.randn(in_dim), requires_grad=True)\n",
    "        # self.register_parameter(\"mask_embed\", self.mask_embed) # Not strictly needed if assigned to self\n",
    "\n",
    "        # Hidden Layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim, aggr='mean'))\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Output Layer: Hidden -> Out\n",
    "        self.convs.append(SAGEConv(hidden_dim, out_dim, aggr='mean'))\n",
    "        self.bns.append(BatchNorm(out_dim))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 1. Masking Strategy\n",
    "        # We clone x so we don't modify the original dataset in memory\n",
    "        # 'detach()' stops gradients flowing back to raw features (which are fixed anyway)\n",
    "        x = x.clone().detach()\n",
    "        \n",
    "        # Replace the first node in the batch with the learnable mask token\n",
    "        # Note: In LinkNeighborLoader, node 0 is usually a target node for the batch\n",
    "        x[0] = self.mask_embed\n",
    "\n",
    "        # 2. Message Passing Loop\n",
    "        for i in range(self.num_layers):\n",
    "            h = self.convs[i](x, edge_index)\n",
    "            h = self.bns[i](h)\n",
    "        \n",
    "            # Activation & Dropout (except for last layer)\n",
    "            if i != self.num_layers - 1:\n",
    "                h = F.relu(h)\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "            # Residual Connection\n",
    "            # Only apply if shapes match (Input Dim != Hidden Dim on layer 0)\n",
    "            if x.shape == h.shape:\n",
    "                h = h + x\n",
    "            \n",
    "            x = h\n",
    "\n",
    "        return x\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Contrastive Training Loop\n",
    "# We use LinkNeighborLoader to generate Positive (Real) and Negative (Fake) edges.\n",
    "\n",
    "# %%\n",
    "def train():\n",
    "    # --- Load Data ---\n",
    "    dataset, data = load_data()\n",
    "    \n",
    "    # --- Setup Loader ---\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']\n",
    "    \n",
    "    # Filter for training edges only\n",
    "    src, _ = data.edge_index\n",
    "    train_mask = torch.isin(src, train_idx)\n",
    "    train_edge_index = data.edge_index[:, train_mask]\n",
    "    \n",
    "    print(f\"Training on {train_edge_index.shape[1]} edges.\")\n",
    "\n",
    "    loader = LinkNeighborLoader(\n",
    "        data=data,\n",
    "        num_neighbors=[10, 10, 5],  # 2-hop neighborhoods\n",
    "        edge_label_index=train_edge_index,\n",
    "        neg_sampling_ratio=1.0, # 1 Neg for 1 Pos\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4, # Increase for faster loading\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # --- Setup Model ---\n",
    "    model = EmbedderGNNv2(\n",
    "        in_dim=INPUT_DIM, \n",
    "        hidden_dim=HIDDEN_DIM, \n",
    "        out_dim=OUTPUT_DIM,\n",
    "        num_layers=4\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # BCEWithLogitsLoss combines Sigmoid + BCE (Numerically stable)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. Forward Pass\n",
    "            # Get embeddings for ALL nodes in the subgraph\n",
    "            z = model(batch.x, batch.edge_index)\n",
    "\n",
    "            # 2. Extract Embeddings for Contrastive Pairs\n",
    "            # edge_label_index contains the pairs we want to score (Pos + Neg)\n",
    "            # Row 0 = Source, Row 1 = Destination\n",
    "            src_emb = z[batch.edge_label_index[0]]\n",
    "            dst_emb = z[batch.edge_label_index[1]]\n",
    "\n",
    "            # 3. Calculate Similarity (Dot Product)\n",
    "            # We sum along dim 1 to get a single score per pair\n",
    "            scores = (src_emb * dst_emb).sum(dim=-1)\n",
    "\n",
    "            # 4. Loss\n",
    "            # Compare scores to labels (1 for real, 0 for fake)\n",
    "            loss = criterion(scores, batch.edge_label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f\"Epoch {epoch} Avg Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c00be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "def run_gpu_inference(input_json, output_pt):\n",
    "    # 1. Load the clean strings\n",
    "    with open(input_json, 'r') as f:\n",
    "        texts = json.load(f)\n",
    "\n",
    "    # 2. Load Model with 4090-specific settings\n",
    "    print(\"Loading Qwen-4B with Flash Attention 2...\")\n",
    "    model = SentenceTransformer(\n",
    "        \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        trust_remote_code=True,\n",
    "        model_kwargs={\n",
    "            \"attn_implementation\": \"flash_attention_2\", \n",
    "            \"torch_dtype\": torch.float16  # Vital for 4090 speed/VRAM\n",
    "        }\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    # 3. Use Multi-Process Pool for Tokenization\n",
    "    # This prevents the CPU from being the bottleneck while the GPU works\n",
    "    pool = model.start_multi_process_pool()\n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} papers...\")\n",
    "    # batch_size=128 is the sweet spot for 4B models on a 24GB card\n",
    "    embeddings_raw = model.encode_multi_process(\n",
    "        texts, \n",
    "        pool, \n",
    "        batch_size=64, \n",
    "        chunk_size=1000\n",
    "    )\n",
    "\n",
    "    model.stop_multi_process_pool(pool)\n",
    "\n",
    "    # 4. Convert to Torch, Truncate (MRL), and Re-normalize\n",
    "    print(\"Moving to GPU for MRL Truncation & Normalization...\")\n",
    "    embeddings = torch.from_numpy(embeddings_raw).to(\"cuda\")\n",
    "\n",
    "    # Slice the first 256 dimensions (The MRL Core)\n",
    "    embeddings_256 = embeddings[:, :256]\n",
    "\n",
    "    # RE-NORMALIZE is mandatory after slicing\n",
    "    embeddings_256 = F.normalize(embeddings_256, p=2, dim=1)\n",
    "\n",
    "    # 5. Save\n",
    "    print(f\"Saving to {output_pt}...\")\n",
    "    torch.save(embeddings_256.cpu(), output_pt)\n",
    "    print(f\"Final Tensor Shape: {embeddings_256.shape}. All done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_gpu_inference('reconstructed_texts.json', 'qwen_embeddings_256.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ariadne_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
